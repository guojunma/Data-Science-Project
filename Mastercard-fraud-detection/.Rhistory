LOGIT <- glm(`Fraud Indicator (Yes/No)` ~ `Risk Assessment`  + `Transaction Value`+ `Card Present Status` + `Chip Usage` + `Cross-border Transaction (Yes/No)` , data = BOLT, family = "binomial")
summary(LOGIT)
summary(LOGIT)
design_train <- model.matrix(~  `Risk Assessment`+ `Transaction Value`  + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_train)
design_test <- model.matrix(~ `Risk Assessment`+ `Transaction Value`  + `Cross-border Transaction (Yes/No)` +  `Card Present Status` + `Chip Usage` , BOLT_test)
set.seed(421)
response_test = BOLT_test$`Fraud Indicator (Yes/No)`
#Train on the entire training set, using the best parameter found during cross validation.
EN_model <- glmnet(x = design_train, y = response_train, family = "binomial", alpha = 0, lambda = 0)
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
prob_predict <- predict(EN_model, type = "response", newx = design_test)
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(prob_predict, response_test)
perf <- performance(tmp_predictions, "tpr", "fpr")
plot(perf,
avg= "threshold",
colorize=TRUE,
lwd= 3,
main= "ROC curve of Elastic net model (AUC = 0.832)")
#Return the AUC
(auc <- performance(tmp_predictions, "auc")@y.values)
tmp_predictions <- ROCR::prediction(prob_predict, response_test)
perf <- performance(tmp_predictions, "tpr", "fpr")
plot(perf,
avg= "threshold",
colorize=TRUE,
lwd= 3,
main= "ROC curve of logistic regression model (AUC = 0.832)")
set.seed(421)
response_test = BOLT_test$`Fraud Indicator (Yes/No)`
#Train on the entire training set, using the best parameter found during cross validation.
EN_model <- glmnet(x = design_train, y = response_train, family = "binomial", alpha = 0.01, lambda = 0.1)
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
prob_predict <- predict(EN_model, type = "response", newx = design_test)
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(prob_predict, response_test)
perf <- performance(tmp_predictions, "tpr", "fpr")
plot(perf,
avg= "threshold",
colorize=TRUE,
lwd= 3,
main= "ROC curve of logistic regression model (AUC = 0.832)")
#Return the AUC
(auc <- performance(tmp_predictions, "auc")@y.values)
plot(perf,
avg= "threshold",
colorize=TRUE,
lwd= 3,
main= "ROC curve of logistic regression model (AUC = 0.832)")
pr <- list(pr_TBB, pr_RF, pr_EN, pr_FNN, pr_LDA)
comparison <- compare(pr, plot = TRUE, measure = "auc")
print(comparison)
pr <- list(pr_TBB, pr_RF, pr_EN, pr_FNN, pr_LDA)
comparison <- compare(pr, plot = TRUE, measure = "auc")
print(comparison)
roc(pr_EN[[1]])
par(new = TRUE)
roc(pr_EN[[2]], col = "blue")
par(new = TRUE)
roc(pr_EN[[3]], col = "red")
par(new = TRUE)
roc(pr_EN[[4]], col = "green")
par(new = TRUE)
roc(pr_EN[[5]], col = "yellow")
timeseries2 <- BOLT %>%
filter(BOLT$`Fraud Indicator (Yes/No)` == "Yes") %>%
group_by(day(BOLT$`Transaction Date`)) %>%
summarise(total_transactions = sum(`Transaction Value`))
View(LOGIT)
plot(LOGIT)
plot(LOGIT$linear.predictors)
View(EN_model)
predict(EN_model, newx = design_test)
predict(EN_model, newx = design_test, type = "class")
pred_class <- predict(EN_model, newx = design_test, type = "class")
plot(pred_class)
plot(as.numeric(pred_class) - 1 )
plot(as.numeric(pred_class) - 1 )
View(pred_class)
plot(as.numeric(pred_class))
plot(as.numeric(pred_class))
ggplot(pred_class)
ggplot(pred_class)
ggplogt(pred_class, aes = (y = s0))
ggplot(pred_class, aes = (y = s0))
set.seed(421)
response_test = BOLT_test$`Fraud Indicator (Yes/No)`
#Train on the entire training set, using the best parameter found during cross validation.
EN_model <- glmnet(x = design_train, y = response_train, family = "binomial", alpha = 0.01, lambda = 0.1)
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
prob_predict <- predict(EN_model, type = "response", newx = design_test)
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(prob_predict, response_test)
perf <- performance(tmp_predictions, "tpr", "fpr")
plot(perf,
avg= "threshold",
colorize=TRUE,
lwd= 3,
main= "ROC curve of logistic regression model (AUC = 0.832)")
#Return the AUC
(auc <- performance(tmp_predictions, "auc")@y.values)
set.seed(421)
response_test = BOLT_test$`Fraud Indicator (Yes/No)`
#Train on the entire training set, using the best parameter found during cross validation.
EN_model <- glmnet(x = design_train, y = response_train, family = "binomial")
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
prob_predict <- predict(EN_model, type = "response", newx = design_test)
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(prob_predict, response_test)
set.seed(421)
response_test = BOLT_test$`Fraud Indicator (Yes/No)`
#Train on the entire training set, using the best parameter found during cross validation.
EN_model <- glmnet(x = design_train, y = response_train, family = "binomial")
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
prob_predict <- predict(EN_model, type = "response", newx = design_test)
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(prob_predict, response_test)
set.seed(421)
response_test = BOLT_test$`Fraud Indicator (Yes/No)`
#Train on the entire training set, using the best parameter found during cross validation.
EN_model <- glmnet(x = design_train, y = response_train, family = "binomial")
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
prob_predict <- predict(EN_model, type = "response", newx = design_test)
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(prob_predict, response_test)
(auc <- performance(tmp_predictions, "auc")@y.values)
BOLT_subset <- filter(`Fraud indicator (Yes/No)` == "Yes")
BOLT_subset <- filter(BOLT$`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset <- filter(BOLT,`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset <- filter(BOLT,BOLT$`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset <- filter(BOLT_train,BOLT$`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset <- BOLT_train %>% filter(`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset <- BOLT_train %>% filter(`Fraud Indicator (Yes/No)` == "Yes")
library(readxl)
library(dplyr)
library(ggplot2)
library(CMA) #For fitting and comparing classification model
library(lubridate)
library(glmnet)
library(ROCR)
BOLT_subset <- BOLT_train %>% filter(`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset <- BOLT_train %>% filter(`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset2 <- BOLT_train %>% filter(`Fraud Indicator (Yes/No)` == "No")
samples <- sample(nrow(BOLT_subset2), nrow(BOLT_subset))
BOLT_subset2 <- BOLT_subset2[samples, ]
BOLT_subset <- rbind(BOLT_subset, BOLT_subset2)
BOLT_subset <- BOLT_train %>% filter(`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset2 <- BOLT_train %>% filter(`Fraud Indicator (Yes/No)` == "No")
samples <- sample(nrow(BOLT_subset2), nrow(BOLT_subset))
BOLT_subset2 <- BOLT_subset2[samples, ]
BOLT_subset <- rbind(BOLT_subset, BOLT_subset2)
BOLT_subset <- BOLT_train %>% filter(`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset2 <- BOLT_train %>% filter(`Fraud Indicator (Yes/No)` == "No")
samples <- sample(nrow(BOLT_subset2), nrow(BOLT_subset))
BOLT_subset2 <- BOLT_subset2[samples, ]
BOLT_subset <- rbind(BOLT_subset, BOLT_subset2)
#Build a design matrix
design_train <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction(Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_subset)
View(BOLT_subset)
colnames(BOLT_subset)
BOLT_subset <- BOLT_train %>% filter(`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset2 <- BOLT_train %>% filter(`Fraud Indicator (Yes/No)` == "No")
samples <- sample(nrow(BOLT_subset2), nrow(BOLT_subset))
BOLT_subset2 <- BOLT_subset2[samples, ]
BOLT_subset <- rbind(BOLT_subset, BOLT_subset2)
#Build a design matrix
design_train <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_subset)
design_test <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_test)
response_train <- BOLT_subset$`Fraud Indicator (Yes/No)`
response_test <- BOLT_test$`Fraud Indicator (Yes/No)
response_test <- BOLT_test$`Fraud Indicator (Yes/No)`
set.seed(421)
#Train on the entire training set, using the best parameter found during cross validation.
EN_model <- glmnet(x = design_train, y = response_train, family = "binomial")
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
prob_predict <- predict(EN_model, type = "response", newx = design_test)
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(prob_predict, response_test)
prob_predict
set.seed(421)
#Train on the entire training set, using the best parameter found during cross validation.
EN_model <- glm(x = design_train, y = response_train, family = "binomial")
set.seed(421)
#Train on the entire training set, using the best parameter found during cross validation.
logit_model <- glm(response_train ~ design_train, family = "binomial")
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
logit_predict <- predict(EN_model, type = "response", newx = design_test)
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(prob_predict, response_test)
logit_predic
logit_predict
logit_model <- glm(response_train ~ design_train, family = "binomial")
logit_predict <- predict(logit_model
, type = "response", newx = design_test)
logit_predict
tmp_predictions <- ROCR::prediction(logit_predict, response_test)
length(response_test)
logit_predict <- predict(logit_model, type = "response", newx = design_test)
View(design_test)
logit_predict <- predict(logit_model, newx = design_test)
View(design_test)
logit_predict <- predict(logit_model, newdata = design_test)
View(logit_model)
View(logit_model)
set.seed(421)
#Train on the entire training set, using the best parameter found during cross validation.
logit_model <- glm(response_train ~ design_train, family = "binomial")
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
logit_predict <- predict(logit_model, newdata = BOLT_test)
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(logit_predict, response_test)
logit_predict <- predict(logit_model, newdata = BOLT_test)
logit_predict <- predict(logit_model, newdata = BOLT_test)
logit_predict <- predict(logit_model, newdata = BOLT_test)
?predict
logit_predict <- predict(logit_model, newdata = BOLT_test)
set.seed(421)
#Train on the entire training set, using the best parameter found during cross validation.
logit_model <- glm(`Fraud Indicator (Yes/No)` ~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage`, data = BOLT_subset, family = "binomial")
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
logit_predict <- predict(logit_model, newdata = BOLT_test)
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(logit_predict, response_test)
perf <- performance(tmp_predictions, "tpr", "fpr")
plot(perf,
avg= "threshold",
colorize=TRUE,
lwd= 3,
main= "ROC curve of logistic regression model")
#Return the AUC
(auc <- performance(tmp_predictions, "auc")@y.values)
set.seed(421)
#Train on the entire training set, using the best parameter found during cross validation.
logit_model <- glm(`Fraud Indicator (Yes/No)` ~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage`, data = BOLT_subset, family = "binomial")
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
logit_predict <- predict(logit_model, newdata = BOLT_test, type = "response")
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(logit_predict, response_test)
perf <- performance(tmp_predictions, "tpr", "fpr")
plot(perf,
avg= "threshold",
colorize=TRUE,
lwd= 3,
main= "ROC curve of logistic regression model")
#Return the AUC
(auc <- performance(tmp_predictions, "auc")@y.values)
library(readxl)
library(dplyr)
library(ggplot2)
library(CMA) #For fitting and comparing classification model
library(lubridate)
library(glmnet)
library(ROCR)
#Import the data
BOLT_Data_Set <- read_excel("BOLT Data Set.xlsx")
dim(BOLT_Data_Set)
head(BOLT_Data_Set)
#Convert the date type from characters to factors
BOLT <- BOLT_Data_Set %>%
filter(!(`Cross-border Transaction (Yes/No)` == 0)) %>%
mutate_if(is.character, as.factor)
timeseries <- BOLT %>%
group_by(`Transaction Date`) %>%
summarise(total_transactions = sum(`Transaction Value`))
ggplot(timeseries, aes(x = `Transaction Date`, y = total_transactions)) +
geom_point() +  # Add scatter plot points
geom_smooth(method = "loess", se = TRUE, color = "red") +  # Add a trend line
labs(title = "Total Transactions Over Time", y = "Total Transactions in Dollars")
timeseries2 <- BOLT %>%
group_by(day(BOLT$`Transaction Date`)) %>%
summarise(total_transactions = sum(`Transaction Value`))
barplot(timeseries2$total_transactions ~ timeseries2$`day(BOLT$\`Transaction Date\`)`,
xlab = "Day of the month", ylab = "Total transaction", main = "Daily total transaction", col = c("red","blue", "green"))
timeseries <- BOLT %>%
filter(BOLT$`Fraud Indicator (Yes/No)` == "Yes") %>%
group_by(days`Transaction Date`) %>%
BOLT_subset <- BOLT_train %>% filter(`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset <- BOLT %>% filter(`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset2 <- BOLT %>% filter(`Fraud Indicator (Yes/No)` == "No")
samples <- sample(nrow(BOLT_subset2), nrow(BOLT_subset))
BOLT_subset2 <- BOLT_subset2[samples, ]
BOLT_subset <- rbind(BOLT_subset, BOLT_subset2)
#Build a design matrix
design_train <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_subset)
response_train <- BOLT_subset$`Fraud Indicator (Yes/No)`
library(CMA)
#Partition the training set to 10 in order perform cross-validation. By setting the argument strat = True, we ensure each split contains the same number of fraud and non-fraud samples.
set.seed(123)
splits <- CMA::GenerateLearningsets(y = BOLT_train$`Fraud Indicator (Yes/No)`, method="CV", fold = 5, strat= TRUE)
library(CMA)
#Partition the training set to 10 in order perform cross-validation. By setting the argument strat = True, we ensure each split contains the same number of fraud and non-fraud samples.
set.seed(123)
splits <- CMA::GenerateLearningsets(y = BOLT_subset$`Fraud Indicator (Yes/No)`, method="CV", fold = 5, strat= TRUE)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
View(design_train)
View(splits)
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
pr_LDA <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = ldaCMA)
nrow(deign_train)
nrow(design_train)
length(response_train)
design_train <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_subset)
nrow(design_train)
BOLT_subset <- BOLT %>% filter(`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset2 <- BOLT %>% filter(`Fraud Indicator (Yes/No)` == "No")
samples <- sample(nrow(BOLT_subset2), nrow(BOLT_subset))
BOLT_subset2 <- BOLT_subset2[samples, ]
BOLT_subset <- rbind(BOLT_subset, BOLT_subset2)
BOLT_subset <- BOLT_subset[complete.cases(BOLT_subset), ]
#Build a design matrix
design_train <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_subset)
response_train <- BOLT_subset$`Fraud Indicator (Yes/No)`
nrow(design_train)
length(response_train)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
#Partition the training set to 10 in order perform cross-validation. By setting the argument strat = True, we ensure each split contains the same number of fraud and non-fraud samples.
set.seed(123)
splits <- CMA::GenerateLearningsets(y = BOLT_subset$`Fraud Indicator (Yes/No)`, method="CV", fold = 5, strat= TRUE)
#Partition the training set to 10 in order perform cross-validation. By setting the argument strat = True, we ensure each split contains the same number of fraud and non-fraud samples.
set.seed(123)
splits <- CMA::GenerateLearningsets(y = response_train, method="CV", fold = 5)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
pr_KNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = knnCMA)
pr_KNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = knnCMA, probability = TRUE)
#Linear discriminant analysis
pr_LDA <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = ldaCMA)
# Elastic Net
tune_EL <- tune(X = design_train, y = response_train, learningsets = splits, classifier = ElasticNetCMA)
pr_EN <- classification(X = design_train, y = response_train, tuneres = tune_EL, learningsets = splits, classifier = ElasticNetCMA)
best(tune_EL)
#Random forest
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = rfCMA)
pr_RF <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = rfCMA)
#Tree-based Gradient Boosting
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = gbmCMA)
pr_TBB <- classification(X = design_train[,-1], y = response_train, tuneres = tune, learningsets = splits, classifier = gbmCMA)
#SVM
pr_SVM <- classification(X = design_train, y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial")
best(tune)
#SVM
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial")
pr_SVM <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial")
best(tune)
#Feed-Forward Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = nnetCMA)
pr_FNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = nnetCMA)
?tune
#Probabilistic Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = pnnCMA)
best(tune)
#Probabilistic Neural Networks
tune <- tune(X = design_train[, -1], y = response_train, learningsets = splits, classifier = pnnCMA)
pr_FNN <- classification(X = design_train[, -1], y = response_train, tuneres = tune, learningsets = splits, classifier = pnnCMA)
best(tune)
View(pr_FNN)
#Probabilistic Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = pnnCMA)
#Probabilistic Neural Networks
tune <- tune(X = design_train[, -1], y = response_train, learningsets = splits, classifier = pnnCMA)
pr_FNN <- classification(X = design_train[, -1], y = response_train, tuneres = tune, learningsets = splits, classifier = pnnCMA)
best(tune)
View(pr_SVM)
#SVM
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial")
pr_SVM <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial", probability = TRUE)
View(tune)
View(pr_SVM)
#probabilistic K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = pknnCMA)
pr_PKNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = pknnCMA)
pr_NB <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = dldaCMA)
View(pr_LDA)
#Tree-based Gradient Boosting
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = gbmCMA)
pr_TGB <- classification(X = design_train[,-1], y = response_train, tuneres = tune, learningsets = splits, classifier = gbmCMA)
rm(pr_TBB)
#Probabilistic Neural Networks
tune <- tune(X = design_train[, -1], y = response_train, learningsets = splits, classifier = pnnCMA)
#Feed-Forward Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = nnetCMA)
pr_FNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = nnetCMA)
#Probabilistic Neural Networks
tune <- tune(X = design_train[, -1], y = response_train, learningsets = splits, classifier = pnnCMA)
#Probabilistic Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = pnnCMA)
#Probabilistic Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = pnnCMA)
#Probabilistic Neural Networks
tune <- tune(X = design_train[, -1], y = response_train, learningsets = splits, classifier = pnnCMA)
pr_PNN <- classification(X = design_train[, -1], y = response_train, tuneres = tune, learningsets = splits, classifier = pnnCMA)
pr <- list(pr_EN, pr_FNN, pr_LDA, pr_NB, pr_PNN, pr_RF, pr_SVM, pr_TGB)
comparison <- compare(pr, plot = TRUE, measure = c("misclassification", "sensitivity", "specificity", "auc"))
View(pr_FNN)
View(pr_NB)
View(pr_LDA)
View(pr_PNN)
View(pr_SVM)
View(pr_TGB)
pr <- list(pr_EN, pr_FNN, pr_LDA, pr_NB, pr_PNN, pr_RF, pr_TGB)
comparison <- compare(pr, plot = TRUE, measure = c("misclassification", "sensitivity", "specificity", "auc"))
pr <- list(pr_EN, pr_FNN, pr_LDA, pr_NB, pr_PNN, pr_RF, pr_SVM, pr_TGB)
comparison <- compare(pr, plot = TRUE, measure = c("misclassification", "sensitivity", "specificity"))
print(comparison)
pr <- list(pr_EN, pr_RF, pr_EN, pr_FNN, pr_LDA)
comparison <- compare(pr, plot = TRUE, measure = "auc")
print(comparison)
pr <- list(pr_EN, pr_RF, pr_FNN, pr_LDA, pr_NB)
comparison <- compare(pr, plot = TRUE, measure = "auc")
print(comparison)
pr <- list(pr_EN, pr_RF, pr_FNN, pr_LDA, pr_NB, pr_PNN)
comparison <- compare(pr, plot = TRUE, measure = "auc")
pr <- list(pr_EN, pr_FNN, pr_LDA, pr_NB, pr_RF, pr_SVM, pr_TGB)
comparison <- compare(pr, plot = TRUE, measure = c("misclassification", "sensitivity", "specificity", "auc"))
print(comparison)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = pknnCMA)
#K Nearest Neighborhood
tune<- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = pknnCMA)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
pr_KNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = knnCMA)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
pr_KNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = knnCMA)
#Linear discriminant analysis
pr_LDA <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = ldaCMA)
#Naive Bayes
pr_NB <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = dldaCMA)
# Elastic Net
tune_EL <- tune(X = design_train, y = response_train, learningsets = splits, classifier = ElasticNetCMA)
pr_EN <- classification(X = design_train, y = response_train, tuneres = tune_EL, learningsets = splits, classifier = ElasticNetCMA)
#Random forest
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = rfCMA)
pr_RF <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = rfCMA)
#Tree-based Gradient Boosting
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = gbmCMA)
pr_TGB <- classification(X = design_train[,-1], y = response_train, tuneres = tune, learningsets = splits, classifier = gbmCMA)
#SVM
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial")
pr_SVM <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial", probability = TRUE)
#Feed-Forward Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = nnetCMA)
pr_FNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = nnetCMA)
?splits
#Partition the training set to 10 in order perform cross-validation. By setting the argument strat = True, we ensure each split contains the same number of fraud and non-fraud samples.
set.seed(123)
splits <- CMA::GenerateLearningsets(y = response_train, method="CV", fold = 5, niter = 10)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
pr_KNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = knnCMA)
#Linear discriminant analysis
pr_LDA <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = ldaCMA)
#Naive Bayes
pr_NB <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = dldaCMA)
# Elastic Net
tune_EL <- tune(X = design_train, y = response_train, learningsets = splits, classifier = ElasticNetCMA)
pr_EN <- classification(X = design_train, y = response_train, tuneres = tune_EL, learningsets = splits, classifier = ElasticNetCMA)
#Random forest
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = rfCMA)
pr_RF <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = rfCMA)
#Partition the training set to 10 in order perform cross-validation. By setting the argument strat = True, we ensure each split contains the same number of fraud and non-fraud samples.
set.seed(123)
splits <- CMA::GenerateLearningsets(y = response_train, method="CV", fold = 5, niter = 5)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
pr_KNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = knnCMA)
#Linear discriminant analysis
pr_LDA <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = ldaCMA)
#Naive Bayes
pr_NB <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = dldaCMA)
# Elastic Net
tune_EL <- tune(X = design_train, y = response_train, learningsets = splits, classifier = ElasticNetCMA)
pr_EN <- classification(X = design_train, y = response_train, tuneres = tune_EL, learningsets = splits, classifier = ElasticNetCMA)
#Random forest
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = rfCMA)
pr_RF <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = rfCMA)
#Tree-based Gradient Boosting
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = gbmCMA)
pr_TGB <- classification(X = design_train[,-1], y = response_train, tuneres = tune, learningsets = splits, classifier = gbmCMA)
#SVM
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial")
pr_SVM <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial", probability = TRUE)
#Feed-Forward Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = nnetCMA)
pr_FNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = nnetCMA)
pr <- list(pr_EN, pr_FNN, pr_LDA, pr_NB, pr_RF, pr_SVM, pr_TGB)
comparison <- compare(pr, plot = TRUE, measure = c("misclassification", "sensitivity", "specificity", "auc"))
print(comparison)
roc(pr_EN[[1]])
par(new = TRUE)
roc(pr_EN[[2]], col = "blue")
par(new = TRUE)
roc(pr_EN[[3]], col = "red")
par(new = TRUE)
roc(pr_EN[[4]], col = "green")
par(new = TRUE)
roc(pr_EN[[5]], col = "yellow")
View(pr_RF)
View(pr_KNN)
save.image("C:/Users/Administrator/Documents/GitHub/Mastercard fraud detection/.RData")
set.seed(421)
#Train on the entire training set, using the best parameter found during cross validation.
logit_model <- glm(`Fraud Indicator (Yes/No)` ~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage`, data = BOLT_subset, family = "binomial")
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
logit_predict <- predict(logit_model, newdata = BOLT_test, type = "response")
coef(logit_model)
summary(logit_model)
