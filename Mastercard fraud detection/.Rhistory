logit_predict <- predict(logit_model, type = "response", newx = design_test)
View(design_test)
logit_predict <- predict(logit_model, newx = design_test)
View(design_test)
logit_predict <- predict(logit_model, newdata = design_test)
View(logit_model)
View(logit_model)
set.seed(421)
#Train on the entire training set, using the best parameter found during cross validation.
logit_model <- glm(response_train ~ design_train, family = "binomial")
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
logit_predict <- predict(logit_model, newdata = BOLT_test)
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(logit_predict, response_test)
logit_predict <- predict(logit_model, newdata = BOLT_test)
logit_predict <- predict(logit_model, newdata = BOLT_test)
logit_predict <- predict(logit_model, newdata = BOLT_test)
?predict
logit_predict <- predict(logit_model, newdata = BOLT_test)
set.seed(421)
#Train on the entire training set, using the best parameter found during cross validation.
logit_model <- glm(`Fraud Indicator (Yes/No)` ~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage`, data = BOLT_subset, family = "binomial")
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
logit_predict <- predict(logit_model, newdata = BOLT_test)
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(logit_predict, response_test)
perf <- performance(tmp_predictions, "tpr", "fpr")
plot(perf,
avg= "threshold",
colorize=TRUE,
lwd= 3,
main= "ROC curve of logistic regression model")
#Return the AUC
(auc <- performance(tmp_predictions, "auc")@y.values)
set.seed(421)
#Train on the entire training set, using the best parameter found during cross validation.
logit_model <- glm(`Fraud Indicator (Yes/No)` ~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage`, data = BOLT_subset, family = "binomial")
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
logit_predict <- predict(logit_model, newdata = BOLT_test, type = "response")
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(logit_predict, response_test)
perf <- performance(tmp_predictions, "tpr", "fpr")
plot(perf,
avg= "threshold",
colorize=TRUE,
lwd= 3,
main= "ROC curve of logistic regression model")
#Return the AUC
(auc <- performance(tmp_predictions, "auc")@y.values)
library(readxl)
library(dplyr)
library(ggplot2)
library(CMA) #For fitting and comparing classification model
library(lubridate)
library(glmnet)
library(ROCR)
#Import the data
BOLT_Data_Set <- read_excel("BOLT Data Set.xlsx")
dim(BOLT_Data_Set)
head(BOLT_Data_Set)
#Convert the date type from characters to factors
BOLT <- BOLT_Data_Set %>%
filter(!(`Cross-border Transaction (Yes/No)` == 0)) %>%
mutate_if(is.character, as.factor)
timeseries <- BOLT %>%
group_by(`Transaction Date`) %>%
summarise(total_transactions = sum(`Transaction Value`))
ggplot(timeseries, aes(x = `Transaction Date`, y = total_transactions)) +
geom_point() +  # Add scatter plot points
geom_smooth(method = "loess", se = TRUE, color = "red") +  # Add a trend line
labs(title = "Total Transactions Over Time", y = "Total Transactions in Dollars")
timeseries2 <- BOLT %>%
group_by(day(BOLT$`Transaction Date`)) %>%
summarise(total_transactions = sum(`Transaction Value`))
barplot(timeseries2$total_transactions ~ timeseries2$`day(BOLT$\`Transaction Date\`)`,
xlab = "Day of the month", ylab = "Total transaction", main = "Daily total transaction", col = c("red","blue", "green"))
timeseries <- BOLT %>%
filter(BOLT$`Fraud Indicator (Yes/No)` == "Yes") %>%
group_by(days`Transaction Date`) %>%
BOLT_subset <- BOLT_train %>% filter(`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset <- BOLT %>% filter(`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset2 <- BOLT %>% filter(`Fraud Indicator (Yes/No)` == "No")
samples <- sample(nrow(BOLT_subset2), nrow(BOLT_subset))
BOLT_subset2 <- BOLT_subset2[samples, ]
BOLT_subset <- rbind(BOLT_subset, BOLT_subset2)
#Build a design matrix
design_train <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_subset)
response_train <- BOLT_subset$`Fraud Indicator (Yes/No)`
library(CMA)
#Partition the training set to 10 in order perform cross-validation. By setting the argument strat = True, we ensure each split contains the same number of fraud and non-fraud samples.
set.seed(123)
splits <- CMA::GenerateLearningsets(y = BOLT_train$`Fraud Indicator (Yes/No)`, method="CV", fold = 5, strat= TRUE)
library(CMA)
#Partition the training set to 10 in order perform cross-validation. By setting the argument strat = True, we ensure each split contains the same number of fraud and non-fraud samples.
set.seed(123)
splits <- CMA::GenerateLearningsets(y = BOLT_subset$`Fraud Indicator (Yes/No)`, method="CV", fold = 5, strat= TRUE)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
View(design_train)
View(splits)
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
pr_LDA <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = ldaCMA)
nrow(deign_train)
nrow(design_train)
length(response_train)
design_train <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_subset)
nrow(design_train)
BOLT_subset <- BOLT %>% filter(`Fraud Indicator (Yes/No)` == "Yes")
BOLT_subset2 <- BOLT %>% filter(`Fraud Indicator (Yes/No)` == "No")
samples <- sample(nrow(BOLT_subset2), nrow(BOLT_subset))
BOLT_subset2 <- BOLT_subset2[samples, ]
BOLT_subset <- rbind(BOLT_subset, BOLT_subset2)
BOLT_subset <- BOLT_subset[complete.cases(BOLT_subset), ]
#Build a design matrix
design_train <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_subset)
response_train <- BOLT_subset$`Fraud Indicator (Yes/No)`
nrow(design_train)
length(response_train)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
#Partition the training set to 10 in order perform cross-validation. By setting the argument strat = True, we ensure each split contains the same number of fraud and non-fraud samples.
set.seed(123)
splits <- CMA::GenerateLearningsets(y = BOLT_subset$`Fraud Indicator (Yes/No)`, method="CV", fold = 5, strat= TRUE)
#Partition the training set to 10 in order perform cross-validation. By setting the argument strat = True, we ensure each split contains the same number of fraud and non-fraud samples.
set.seed(123)
splits <- CMA::GenerateLearningsets(y = response_train, method="CV", fold = 5)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
pr_KNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = knnCMA)
pr_KNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = knnCMA, probability = TRUE)
#Linear discriminant analysis
pr_LDA <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = ldaCMA)
# Elastic Net
tune_EL <- tune(X = design_train, y = response_train, learningsets = splits, classifier = ElasticNetCMA)
pr_EN <- classification(X = design_train, y = response_train, tuneres = tune_EL, learningsets = splits, classifier = ElasticNetCMA)
best(tune_EL)
#Random forest
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = rfCMA)
pr_RF <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = rfCMA)
#Tree-based Gradient Boosting
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = gbmCMA)
pr_TBB <- classification(X = design_train[,-1], y = response_train, tuneres = tune, learningsets = splits, classifier = gbmCMA)
#SVM
pr_SVM <- classification(X = design_train, y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial")
best(tune)
#SVM
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial")
pr_SVM <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial")
best(tune)
#Feed-Forward Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = nnetCMA)
pr_FNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = nnetCMA)
?tune
#Probabilistic Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = pnnCMA)
best(tune)
#Probabilistic Neural Networks
tune <- tune(X = design_train[, -1], y = response_train, learningsets = splits, classifier = pnnCMA)
pr_FNN <- classification(X = design_train[, -1], y = response_train, tuneres = tune, learningsets = splits, classifier = pnnCMA)
best(tune)
View(pr_FNN)
#Probabilistic Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = pnnCMA)
#Probabilistic Neural Networks
tune <- tune(X = design_train[, -1], y = response_train, learningsets = splits, classifier = pnnCMA)
pr_FNN <- classification(X = design_train[, -1], y = response_train, tuneres = tune, learningsets = splits, classifier = pnnCMA)
best(tune)
View(pr_SVM)
#SVM
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial")
pr_SVM <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial", probability = TRUE)
View(tune)
View(pr_SVM)
#probabilistic K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = pknnCMA)
pr_PKNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = pknnCMA)
pr_NB <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = dldaCMA)
View(pr_LDA)
#Tree-based Gradient Boosting
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = gbmCMA)
pr_TGB <- classification(X = design_train[,-1], y = response_train, tuneres = tune, learningsets = splits, classifier = gbmCMA)
rm(pr_TBB)
#Probabilistic Neural Networks
tune <- tune(X = design_train[, -1], y = response_train, learningsets = splits, classifier = pnnCMA)
#Feed-Forward Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = nnetCMA)
pr_FNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = nnetCMA)
#Probabilistic Neural Networks
tune <- tune(X = design_train[, -1], y = response_train, learningsets = splits, classifier = pnnCMA)
#Probabilistic Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = pnnCMA)
#Probabilistic Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = pnnCMA)
#Probabilistic Neural Networks
tune <- tune(X = design_train[, -1], y = response_train, learningsets = splits, classifier = pnnCMA)
pr_PNN <- classification(X = design_train[, -1], y = response_train, tuneres = tune, learningsets = splits, classifier = pnnCMA)
pr <- list(pr_EN, pr_FNN, pr_LDA, pr_NB, pr_PNN, pr_RF, pr_SVM, pr_TGB)
comparison <- compare(pr, plot = TRUE, measure = c("misclassification", "sensitivity", "specificity", "auc"))
View(pr_FNN)
View(pr_NB)
View(pr_LDA)
View(pr_PNN)
View(pr_SVM)
View(pr_TGB)
pr <- list(pr_EN, pr_FNN, pr_LDA, pr_NB, pr_PNN, pr_RF, pr_TGB)
comparison <- compare(pr, plot = TRUE, measure = c("misclassification", "sensitivity", "specificity", "auc"))
pr <- list(pr_EN, pr_FNN, pr_LDA, pr_NB, pr_PNN, pr_RF, pr_SVM, pr_TGB)
comparison <- compare(pr, plot = TRUE, measure = c("misclassification", "sensitivity", "specificity"))
print(comparison)
pr <- list(pr_EN, pr_RF, pr_EN, pr_FNN, pr_LDA)
comparison <- compare(pr, plot = TRUE, measure = "auc")
print(comparison)
pr <- list(pr_EN, pr_RF, pr_FNN, pr_LDA, pr_NB)
comparison <- compare(pr, plot = TRUE, measure = "auc")
print(comparison)
pr <- list(pr_EN, pr_RF, pr_FNN, pr_LDA, pr_NB, pr_PNN)
comparison <- compare(pr, plot = TRUE, measure = "auc")
pr <- list(pr_EN, pr_FNN, pr_LDA, pr_NB, pr_RF, pr_SVM, pr_TGB)
comparison <- compare(pr, plot = TRUE, measure = c("misclassification", "sensitivity", "specificity", "auc"))
print(comparison)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = pknnCMA)
#K Nearest Neighborhood
tune<- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = pknnCMA)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
pr_KNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = knnCMA)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
pr_KNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = knnCMA)
#Linear discriminant analysis
pr_LDA <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = ldaCMA)
#Naive Bayes
pr_NB <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = dldaCMA)
# Elastic Net
tune_EL <- tune(X = design_train, y = response_train, learningsets = splits, classifier = ElasticNetCMA)
pr_EN <- classification(X = design_train, y = response_train, tuneres = tune_EL, learningsets = splits, classifier = ElasticNetCMA)
#Random forest
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = rfCMA)
pr_RF <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = rfCMA)
#Tree-based Gradient Boosting
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = gbmCMA)
pr_TGB <- classification(X = design_train[,-1], y = response_train, tuneres = tune, learningsets = splits, classifier = gbmCMA)
#SVM
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial")
pr_SVM <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial", probability = TRUE)
#Feed-Forward Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = nnetCMA)
pr_FNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = nnetCMA)
?splits
#Partition the training set to 10 in order perform cross-validation. By setting the argument strat = True, we ensure each split contains the same number of fraud and non-fraud samples.
set.seed(123)
splits <- CMA::GenerateLearningsets(y = response_train, method="CV", fold = 5, niter = 10)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
pr_KNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = knnCMA)
#Linear discriminant analysis
pr_LDA <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = ldaCMA)
#Naive Bayes
pr_NB <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = dldaCMA)
# Elastic Net
tune_EL <- tune(X = design_train, y = response_train, learningsets = splits, classifier = ElasticNetCMA)
pr_EN <- classification(X = design_train, y = response_train, tuneres = tune_EL, learningsets = splits, classifier = ElasticNetCMA)
#Random forest
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = rfCMA)
pr_RF <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = rfCMA)
#Partition the training set to 10 in order perform cross-validation. By setting the argument strat = True, we ensure each split contains the same number of fraud and non-fraud samples.
set.seed(123)
splits <- CMA::GenerateLearningsets(y = response_train, method="CV", fold = 5, niter = 5)
#K Nearest Neighborhood
tune<- tune(X = design_train, y = response_train, learningsets = splits, classifier = knnCMA)
pr_KNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = knnCMA)
#Linear discriminant analysis
pr_LDA <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = ldaCMA)
#Naive Bayes
pr_NB <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = dldaCMA)
# Elastic Net
tune_EL <- tune(X = design_train, y = response_train, learningsets = splits, classifier = ElasticNetCMA)
pr_EN <- classification(X = design_train, y = response_train, tuneres = tune_EL, learningsets = splits, classifier = ElasticNetCMA)
#Random forest
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = rfCMA)
pr_RF <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = rfCMA)
#Tree-based Gradient Boosting
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = gbmCMA)
pr_TGB <- classification(X = design_train[,-1], y = response_train, tuneres = tune, learningsets = splits, classifier = gbmCMA)
#SVM
tune <- tune(X = design_train[,-1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial")
pr_SVM <- classification(X = design_train[, -1], y = response_train, learningsets = splits, classifier = svmCMA, kernel ="polynomial", probability = TRUE)
#Feed-Forward Neural Networks
tune <- tune(X = design_train, y = response_train, learningsets = splits, classifier = nnetCMA)
pr_FNN <- classification(X = design_train, y = response_train, tuneres = tune, learningsets = splits, classifier = nnetCMA)
pr <- list(pr_EN, pr_FNN, pr_LDA, pr_NB, pr_RF, pr_SVM, pr_TGB)
comparison <- compare(pr, plot = TRUE, measure = c("misclassification", "sensitivity", "specificity", "auc"))
print(comparison)
roc(pr_EN[[1]])
par(new = TRUE)
roc(pr_EN[[2]], col = "blue")
par(new = TRUE)
roc(pr_EN[[3]], col = "red")
par(new = TRUE)
roc(pr_EN[[4]], col = "green")
par(new = TRUE)
roc(pr_EN[[5]], col = "yellow")
View(pr_RF)
View(pr_KNN)
save.image("C:/Users/Administrator/Documents/GitHub/Mastercard fraud detection/.RData")
set.seed(421)
#Train on the entire training set, using the best parameter found during cross validation.
logit_model <- glm(`Fraud Indicator (Yes/No)` ~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage`, data = BOLT_subset, family = "binomial")
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
logit_predict <- predict(logit_model, newdata = BOLT_test, type = "response")
coef(logit_model)
summary(logit_model)
BOLT %>%
filter(`Fraud Indicator (Yes/No)` == "Yes") %>%
summarise(total_value = sum(`Transaction Value`), count = n())
install.packages('unbalanced', dependencies = TRUE)
library(xfun)
install.packages(c("BiocManager", "bitops", "bslib", "cli", "colorspace", "cpp11", "crayon", "curl", "data.table", "digest", "evaluate", "fastmap", "httr2", "knitr", "openssl", "ps", "Rcpp", "RcppEigen", "RCurl", "reprex", "rjson", "rlang", "rmarkdown", "tinytex", "uuid", "withr", "xfun", "XML", "yaml"))
library(readxl)
library(dplyr)
library(ggplot2)
library(CMA)
library(lubridate)
library(glmnet)
library(ROCR)
library(CMA)
#Import the data
BOLT_Data_Set <- read_excel("data/BOLT Data Set.xlsx")
dim(BOLT_Data_Set)
head(BOLT_Data_Set)
#Convert the date type from characters to factors
BOLT <- BOLT_Data_Set %>%
filter(!(`Cross-border Transaction (Yes/No)` == 0)) %>%
mutate_if(is.character, as.factor)
#loos at the summary of the dataset.
summary(BOLT)
spend_location <- BOLT %>%
group_by(`Merchant Location`) %>%
summarise(number_of_transaction = n(), total_transaction_value = sum(`Transaction Value`)) %>%
arrange(desc(total_transaction_value))
top10 <- head(spend_location, 10)
Other <- spend_location[11: nrow(spend_location), ] %>%
summarise(number_of_transaction = sum(number_of_transaction), total_transaction_value = sum(total_transaction_value)) %>%
mutate(`Merchant Location` = "other")
(spend_location <- rbind(top10, Other))
ggplot(spend_location, aes(x = "", y = `total_transaction_value`, fill = `Merchant Location`)) +
geom_bar(stat = "identity", width = 1, color = "white") +
coord_polar("y", start = 0) +
theme_void() +
labs(title = "Total Transaction value ", fill = "Countries")
ggplot(spend_location, aes(x = "", y = `total_transaction_value`, fill = `Merchant Location`)) +
geom_bar(stat = "identity", width = 1, color = "white") +
coord_polar("y", start = 0) +
theme_void() +
labs(title = "Total Transaction value by countries ", fill = "Countries")
BOLT %>%
filter(`Merchant Location` == "USA") %>%
group_by(`Merchant Category Code (MCC)`) %>%
summarise(number_of_transaction = n(), total_transaction_value = sum(`Transaction Value`)) %>%
arrange(desc(total_transaction_value))
timeseries <- BOLT %>%
group_by(`Transaction Date`) %>%
summarise(total_transactions = sum(`Transaction Value`))
ggplot(timeseries, aes(x = `Transaction Date`, y = total_transactions)) +
geom_point() +  # Add scatter plot points
geom_smooth(method = "loess", se = TRUE, color = "red") +  # Add a trend line
labs(title = "Total Transactions Over Time", y = "Total Transactions in Dollars")
timeseries2 <- BOLT %>%
group_by(day(BOLT$`Transaction Date`)) %>%
summarise(total_transactions = sum(`Transaction Value`))
barplot(timeseries2$total_transactions ~ timeseries2$`day(BOLT$\`Transaction Date\`)`,
xlab = "Day of the month", ylab = "Total transaction", main = "Daily total transaction", col = c("red","blue", "green"))
BOLT %>%
filter(`Fraud Indicator (Yes/No)` == "Yes") %>%
summarise(total_value = sum(`Transaction Value`), count = n())
BOLT %>%
group_by(`Merchant Location`) %>%
summarise(num_of_transaction = n(), num_of_fraud = sum(`Fraud Indicator (Yes/No)` == "Yes")) %>%
mutate(proportion_of_fraud = num_of_fraud / num_of_transaction) %>%
arrange(desc(num_of_fraud))
BOLT %>%
group_by(`Merchant Location`) %>%
summarise(num_of_transaction = n(), num_of_fraud = sum(`Fraud Indicator (Yes/No)` == "Yes")) %>%
mutate(proportion_of_fraud = num_of_fraud / num_of_transaction) %>%
arrange(desc(proportion_of_fraud))
BOLT %>%
group_by(`Merchant Location`) %>%
summarise(num_of_transaction = n(), num_of_fraud = sum(`Fraud Indicator (Yes/No)` == "Yes")) %>%
mutate(proportion_of_fraud = num_of_fraud / num_of_transaction) %>%
arrange(desc(proportion_of_fraud))
BOLT %>%
group_by(`Payment Method`) %>%
summarise(num_of_fraud = sum(`Fraud Indicator (Yes/No)` == "Yes")) %>%
arrange(desc(num_of_fraud))
BOLT %>%
group_by(`Cross-border Transaction (Yes/No)`) %>%
summarise(number_of_transaction = n(), fraud = sum(`Fraud Indicator (Yes/No)` == "Yes"), Non_fraud = sum(`Fraud Indicator (Yes/No)` == "No"))
contingency_table <- as.table(rbind(c(91, 174), c(14754, 84863)))
dimnames(contingency_table) <- list(Fraud = c("Yes", "No"), Crossborder = c("Yes","No") )
print(contingency_table)
chisq.test(contingency_table)
BOLT %>%
group_by(`Card Present Status`, `Chip Usage`) %>%
summarise(fraud = sum(`Fraud Indicator (Yes/No)` == "Yes"), Non_fraud = sum(`Fraud Indicator (Yes/No)` == "No"), ratio_of_fraud = fraud / (fraud + Non_fraud) )
contingency_table <- as.table(rbind(c(37, 211), c(45644, 49882)))
dimnames(contingency_table) <- list(Fraud = c("Yes", "No"), ChipUsage = c("Yes","No") )
mosaicplot(contingency_table, main = "Mosaic plot of the table")
chisq.test(contingency_table)
contingency_table <- as.table(rbind(c(37, 211), c(45644, 49882)))
dimnames(contingency_table) <- list(Fraud = c("Yes", "No"), ChipUsage = c("Yes","No") )
chisq.test(contingency_table)
BOLT %>%
group_by(`Fraud Indicator (Yes/No)`) %>%
summarise(`average risk assessment` = mean(`Risk Assessment`, na.rm = TRUE),
`average transaction` = mean(`Transaction Value`, na.rm = TRUE))
par(mfrow = c(1, 2))
boxplot(`Risk Assessment` ~ `Fraud Indicator (Yes/No)`, data = BOLT, main = "Box plot 1", col = c("blue", "green") )
boxplot(`Transaction Value` ~ `Fraud Indicator (Yes/No)`, data = BOLT, main = "Box plot 2", col = c("blue", "green"))
t.test(`Risk Assessment` ~ `Fraud Indicator (Yes/No)`, data = BOLT)
t.test(`Transaction Value` ~ `Fraud Indicator (Yes/No)`, data = BOLT)
set.seed(123)
split <- sample(c(1:nrow(BOLT)), size = 0.8 * nrow(BOLT))
BOLT_train <- BOLT[split, ]
BOLT_test <- BOLT[-split, ]
#Filter out the missing value
BOLT_train <- BOLT_train[complete.cases(BOLT_train), ]
BOLT_test <- BOLT_test[complete.cases(BOLT_test), ]
#Build a design matrix
design_train <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction(Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_train)
View(BOLT_train)
set.seed(123)
split <- sample(c(1:nrow(BOLT)), size = 0.8 * nrow(BOLT))
BOLT_train <- BOLT[split, ]
BOLT_test <- BOLT[-split, ]
#Filter out the missing value
BOLT_train <- BOLT_train[complete.cases(BOLT_train), ]
BOLT_test <- BOLT_test[complete.cases(BOLT_test), ]
#Build a design matrix
design_train <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_train)
design_test <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction(Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_test)
set.seed(123)
split <- sample(c(1:nrow(BOLT)), size = 0.8 * nrow(BOLT))
BOLT_train <- BOLT[split, ]
BOLT_test <- BOLT[-split, ]
#Filter out the missing value
BOLT_train <- BOLT_train[complete.cases(BOLT_train), ]
BOLT_test <- BOLT_test[complete.cases(BOLT_test), ]
#Build a design matrix
design_train <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_train)
design_test <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_test)
response_train <- BOLT_train$`Fraud Indicator (Yes/No)`
response_test <- BOLT_test$`Fraud Indicator (Yes/No)`
#Check if the splits are balanced
table(BOLT_train$`Fraud Indicator (Yes/No)`)
table(BOLT_test$`Fraud Indicator (Yes/No)`)
set.seed(123)
split <- sample(c(1:nrow(BOLT)), size = 0.8 * nrow(BOLT))
BOLT_train <- BOLT[split, ]
BOLT_test <- BOLT[-split, ]
#Filter out the missing value
BOLT_train <- BOLT_train[complete.cases(BOLT_train), ]
BOLT_test <- BOLT_test[complete.cases(BOLT_test), ]
#Build a design matrix
design_train <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_train)
design_test <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_test)
response_train <- BOLT_train$`Fraud Indicator (Yes/No)`
response_test <- BOLT_test$`Fraud Indicator (Yes/No)`
set.seed(123)
split <- sample(c(1:nrow(BOLT)), size = 0.8 * nrow(BOLT))
BOLT_train <- BOLT[split, ]
BOLT_test <- BOLT[-split, ]
#Filter out the missing value
BOLT_train <- BOLT_train[complete.cases(BOLT_train), ]
BOLT_test <- BOLT_test[complete.cases(BOLT_test), ]
#Build a design matrix
design_train <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_train)
design_test <- model.matrix(~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage` , BOLT_test)
response_train <- BOLT_train$`Fraud Indicator (Yes/No)`
response_test <- BOLT_test$`Fraud Indicator (Yes/No)`
#Partition the training set to 10 in order perform cross-validation. By setting the argument strat = True, we ensure each split contains the same number of fraud and non-fraud samples.
set.seed(123)
splits <- CMA::GenerateLearningsets(y = response_train, method="CV", fold = 5, niter = 5)
pr <- list(pr_EN, pr_FNN, pr_LDA, pr_NB, pr_RF, pr_SVM, pr_TGB)
comparison <- compare(pr, plot = TRUE, measure = c("misclassification", "sensitivity", "specificity", "auc"))
print(comparison)
set.seed(421)
#Train on the entire training set, using the best parameter found during cross validation.
logit_model <- glm(`Fraud Indicator (Yes/No)` ~ `Risk Assessment`+ `Transaction Value` + `Cross-border Transaction (Yes/No)` + `Card Present Status` + `Chip Usage`, data = BOLT_subset, family = "binomial")
#Return the Prediction on the test samples - the probability of being fraud/non-fraud
logit_predict <- predict(logit_model, newdata = BOLT_test, type = "response")
#Draw a ROC curve
tmp_predictions <- ROCR::prediction(logit_predict, response_test)
perf <- performance(tmp_predictions, "tpr", "fpr")
plot(perf,
avg= "threshold",
colorize=TRUE,
lwd= 3,
main= "ROC curve of logistic regression model")
#Return the AUC
(auc <- performance(tmp_predictions, "auc")@y.values)
library(fastmap)
install.packages(c("cli", "digest", "fastmap", "rlang"))
library(fastmap)
install.packages(c("cli", "digest", "fastmap", "rlang"))
detach("package:fastmap", unload = TRUE)
library(fastmap)
detach("package:fastmap", unload = TRUE)
library(fastmap)
library(fastmap)
